#! /usr/bin/env python3

description = """
Export LMC recorded data into CSV format, potentially for import into a data
base via bulk transfer (e.g. bcp)

The command requires:
    * a DB specific file describing LMC data type -> DB type mapping
    * a LMC schema file generated by lmcrec-inventory command or, better still
      by lmcrec-schema-merge based on individual inventories
"""


import argparse
import csv
import gzip
import os
import re
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timezone
from shutil import rmtree
from typing import Any, Dict, Iterable, List, Optional, Union
from uuid import uuid4
from zlib import Z_DEFAULT_COMPRESSION, Z_NO_COMPRESSION

import yaml
from cache import LmcrecScanRetCode
from codec import LmcVarType
from config import get_lmcrec_runtime
from misc.timeutils import format_ts
from query import (
    LmcrecQueryIntervalStateCache,
    get_file_selection_arg_parser,
    process_file_selection_args,
)
from tabulate import SEPARATING_LINE, tabulate
from tzlocal import get_localzone

from .help_formatter import CustomWidthFormatter
from .lmcrec_schema import (
    LMCREC_SCHEMA_CLASS_VAR_MAX_SIZE_KEY,
    LMCREC_SCHEMA_CLASS_VAR_NEG_VALS_KEY,
    LMCREC_SCHEMA_CLASS_VAR_TYPE_KEY,
    LMCREC_SCHEMA_CLASSES_KEY,
    LMCREC_SCHEMA_INFO_INST_MAX_SIZE_KEY,
    LMCREC_SCHEMA_INFO_KEY,
    LmcrecSchema,
)

# Default values:
TABLE_NAME_MAX_SIZE_DEFAULT = 30
COL_NAME_MAX_SIZE_DEFAULT = 128
TABLE_NAME_SUFFIX_DEFAULT = "_t"
COL_NAME_SUFFIX_DEFAULT = "_col"

TIMESTAMP_COL_NAME_DEFAULT = "__ts__"
TIMESTAMP_COL_TYPE_DEFAULT = "datetime"
TIMESTAMP_COL_STRFTIME_DEFAULT = "%b %d %Y %I:%M:%S%p"
TIMESTAMP_COL_USE_GMT_DEFAULT = False

INSTANCE_COL_NAME_DEFAULT = "__inst__"
INSTANCE_COL_TYPE_DEFAULT = "varchar({max(inst_max_size, 255)})"

NUMERIC_COL_TYPE_DEFAULT = "int"
LARGE_NUMERIC_COL_TYPE_DEFAULT = "bigint"
STRING_COL_TYPE_DEFAULT = "varchar({max(max_size, 32)})"
BOOLEAN_COL_TYPE_DEFAULT = "tinyint"
BOOL_TRUE_VALUE_DEFAULT = 1
BOOL_FALSE_VALUE_DEFAULT = 1

# DB mapping keys:
DB_MAPPING_TABLE_NAME_MAX_SIZE_KEY = "table_name_max_size"
DB_MAPPING_COL_NAME_MAX_SIZE_KEY = "col_name_max_size"
DB_MAPPING_TABLE_NAME_SUFFIX_KEY = "table_name_suffix"
DB_MAPPING_COL_NAME_SUFFIX_KEY = "col_name_suffix"

DB_MAPPING_TIMESTAMP_COL_KEY = "timestamp_col"
DB_MAPPING_TIMESTAMP_COL_NAME_KEY = "name"
DB_MAPPING_TIMESTAMP_COL_TYPE_KEY = "col_type"
DB_MAPPING_TIMESTAMP_COL_STRFTIME_KEY = "strftime"
DB_MAPPING_TIMESTAMP_COL_USE_GMT_KEY = "use_gmt"

DB_MAPPING_INSTANCE_COL_KEY = "instance_col"
DB_MAPPING_INSTANCE_COL_NAME_KEY = "name"
DB_MAPPING_INSTANCE_COL_DB_TYPE_KEY = "col_type"

DB_DATA_TYPE_MAPPING_KEY = "data_type_mapping"

DB_DATA_TYPE_MAPPING_NUMERIC_CATEGORY_KEY = "numeric"
DB_DATA_TYPE_MAPPING_LARGE_NUMERIC_CATEGORY_KEY = "large_numeric"
DB_DATA_TYPE_MAPPING_STRING_CATEGORY_KEY = "string"
DB_DATA_TYPE_MAPPING_BOOLEAN_CATEGORY_KEY = "boolean"

DB_DATA_TYPE_MAPPING_COL_TYPE_KEY = "col_type"
DB_DATA_TYPE_MAPPING_SIGNED_COL_TYPE_KEY = "signed_col_type"
DB_DATA_TYPE_MAPPING_UNSIGNED_COL_TYPE_KEY = "unsigned_col_type"

DB_DATA_TYPE_MAPPING_TRUE_VALUE_KEY = "true_value"
DB_DATA_TYPE_MAPPING_FALSE_VALUE_KEY = "false_value"

DB_MAPPING_CSV_KEY = "csv"
DB_MAPPING_CSV_DIALECT_KEY = "dialect"
DB_MAPPING_CSV_DIALECT_PARAMS_KEY = "dialect_params"
DB_MAPPING_CSV_INCLUDE_HEADER_KEY = "include_header"
DB_MAPPING_CSV_MAX_ROWS_PER_FILE_KEY = "max_rows_per_file"

DB_MAPPING_BCP_FMT_KEY = "bcp_fmt"
DB_MAPPING_BCP_FMT_VERSION_KEY = "version"
DB_MAPPING_BCP_FMT_HOST_DATA_TYPE_KEY = "host_data_type"
DB_MAPPING_BCP_FMT_HOST_DATA_LENGTH_KEY = "host_data_length"
DB_MAPPING_BCP_FMT_STRING_COLLATION_KEY = "string_collation"

# CSV:
CSV_DIALECT_DEFAULT = "unix"
CSV_INCLUDE_HEADER_DEFAULT = True
CSV_MAX_LINES_PER_FILE_DEFAULT = 10000

# BCP:
BCP_FMT_VERSION_DEFAULT = "10.0"
BCP_FMT_HOST_DATA_TYPE_DEFAULT = "SYBCHAR"
BCP_FMT_HOST_DATA_LENGTH_DEFAULT = 1024
BCP_FMT_STRING_COLLATION_DEFAULT = None
BCP_FILE_FIELD_SEP = "\t"

# Export:
EXPORT_DATA_SUB_DIR = "table-data"
CSV_BATCH_FILE_PREFIX = "batch"
CSV_BATCH_FILE_SUFFIX = ".csv"
GZIP_FILE_SUFFIX = ".gz"
EXPORT_SQL_SUB_DIR = "sql"
CREATE_TABLE_INDENT = " " * 4
MAPPING_INFO_FILE = "lmcrec-db-mapping.txt"
BCP_FMT_FILE = "bcp.fmt"

lmc_var_type_to_category_key = dict()
for vt in [
    LmcVarType.COUNTER,
    LmcVarType.GAUGE,
    LmcVarType.GAUGE_CONFIG,
    LmcVarType.NUMERIC,
    LmcVarType.NUMERIC_CONFIG,
    LmcVarType.NUMERIC_RANGE,
]:
    lmc_var_type_to_category_key[vt.name] = DB_DATA_TYPE_MAPPING_NUMERIC_CATEGORY_KEY
for vt in [
    LmcVarType.LARGE_NUMERIC,
]:
    lmc_var_type_to_category_key[vt.name] = (
        DB_DATA_TYPE_MAPPING_LARGE_NUMERIC_CATEGORY_KEY
    )
for vt in [
    LmcVarType.STRING,
    LmcVarType.STRING_CONFIG,
]:
    lmc_var_type_to_category_key[vt.name] = DB_DATA_TYPE_MAPPING_STRING_CATEGORY_KEY
for vt in [
    LmcVarType.BOOLEAN,
    LmcVarType.BOOLEAN_CONFIG,
]:
    lmc_var_type_to_category_key[vt.name] = DB_DATA_TYPE_MAPPING_BOOLEAN_CATEGORY_KEY


class SchemaNormalizer:
    """Convert LMC names into DB acceptable ones"""

    def __init__(
        self,
        max_len: int = 0,
        reserved_names: Optional[Iterable[str]] = None,
        suffix: str = "",
    ):
        """Create normalizer

        Args:
            max_len (int):
                If > 0 then the max length limit for the word, post
                normalization. This includes the potential disambiguation count
                appended after the word and the suffix, if any.

            reserved_names (Iterable[str]):
                The list of names that cannot be the result of normalization,
                for instance pre-defined tables and/or columns. They will be
                used to seed the repetition count. If suffix is in use and if a
                reserved name ends with it, the primer entry is the word without
                the suffix. For instance if suffix == "_col" and
                "my_special_col" is a reserved name, the reserved primer entry
                will be "my_special_col".

            suffix (str):
                An optional suffix to append to the normalized word. This is
                useful to avoid clashes with SQL key words (select, from,
                create, table, etc) without having to include them all into the
                reserved_names. Suffix should contain only the [-zA-Z0-9_]
                characters.
        """

        if suffix:
            assert not re.search(
                r"[^a-zA-Z0-9_]", suffix
            ), "suffix {suffix!r} has illegal characters (not a-zA-Z0-9_)"
            suffix = suffix.lower()
        else:
            suffix = ""
        self._max_len = max_len - len(suffix)
        self._repetition_count = defaultdict(int)
        if reserved_names:
            for name in reserved_names:
                if suffix and name.lower().endswith(suffix):
                    name = name[: -len(suffix)]
                self._repetition_count[name] = 1
        self._suffix = suffix

    def __call__(self, word: str) -> str:
        normalized_word = word

        # Pattern rules:
        for pattern, repl in [
            # '# of word -> 'no_of_word':
            (r"^\s*#\s*of\s+", r"no_of_"),
            # '# word' -> 'no_of_word':
            (r"^\s*#\s*", r"no_of_"),
            # ACRONYMWord -> 'ACRONYM_Word':
            (r"([A-Z]{2,})([A-Z][a-z])", r"\1_\2"),
            # camel[number]Case -> camel[number]_Case:
            (r"([a-z0-9])([A-Z])", r"\1_\2"),
            # drop ending clusters of non-standard chars:
            (r"[^a-zA-Z0-9_]+$", r""),
            # clusters of non-standard chars -> '_':
            (r"[^a-zA-Z0-9_]+", r"_"),
            # compress multiple '_' into one:
            (r"_{2,}", "_"),
            # drop starting _ followed by letter:
            (r"^_([a-z])", r"\1"),
            # drop ending '_':
            (r"_$", ""),
            # if it starts w/ digit, prepend '_':
            (r"^([0-9])", r"_\1"),
        ]:
            normalized_word = re.sub(pattern, repl, normalized_word)

        # Lowercase:
        normalized_word = normalized_word.lower()

        # Suffix check:
        suffix = self._suffix
        if suffix:
            if normalized_word.endswith(suffix):
                normalized_word = normalized_word[: -len(suffix)]
                if normalized_word.endswith("_") and suffix.startswith("_"):
                    normalized_word = normalized_word[:-1]

        # Length limit:
        if self._max_len > 0 and len(normalized_word) > self._max_len:
            normalized_word = normalized_word[: self._max_len]
            if normalized_word.endswith("_") and suffix.startswith("_"):
                normalized_word = normalized_word[:-1]

        # Disambiguation:
        self._repetition_count[normalized_word] += 1
        rep_count = self._repetition_count[normalized_word]
        if rep_count > 1:
            while True:
                candidate = normalized_word + str(rep_count)
                if self._max_len > 0 and len(candidate) > self._max_len:
                    normalized_word = normalized_word[:-1]
                elif candidate in self._repetition_count:
                    rep_count += 1
                else:
                    normalized_word = candidate
                    break
            self._repetition_count[candidate] += 1
            self._repetition_count[normalized_word] = rep_count
            normalized_word = candidate

        # Apply suffix:
        if suffix:
            normalized_word += suffix
        return normalized_word


def export_csv_dialect(
    name: str = "lmcrec_export",
    reference: Optional[str] = CSV_DIALECT_DEFAULT,
    **params,
) -> csv.Dialect:
    supported_params = set(
        attr for attr in csv.excel().__dir__() if not attr.startswith("_")
    )

    register_params = dict()
    if reference:
        ref_dialect = csv.get_dialect(reference)
        for attr in ref_dialect.__dir__():
            if attr in supported_params:
                register_params[attr] = getattr(ref_dialect, attr)
    for param, val in params.items():
        if param not in supported_params:
            continue
        if param == "quoting":
            val = val.upper()
            if not val.startswith("QUOTE_"):
                val = "QUOTE_" + val
            val = getattr(csv, val)
        register_params[param] = val
    csv.register_dialect(name, **register_params)
    return csv.get_dialect(name)


class LmcrecDbMapping:

    def __init__(self, lmcrec_schema: LmcrecSchema, db_mapping: Optional[dict] = None):
        """Initialize the mapping object

        Important members:

        .datetime_from_ts(ts)

        .bool_val(val)

        .tables[table_name] = [
            (col_name, col_type, needs_collation), (col_name, col_type, needs_collation), ...
        ]                                  ^
        .lmc_classes[class_name] =         |
                table_name,                |
                {                          |
                    var_name: (var_type, col_index),
                }
            )
        }

        .csv_dialect
        .csv_include_header
        .csv_max_rows_per_file

        .bcp_version
        .bcp_host_data_type
        .bcp_host_data_length
        .bcp_string_collation
        """

        if db_mapping is None:
            db_mapping = dict()

        table_name_max_size = db_mapping.get(
            DB_MAPPING_TABLE_NAME_MAX_SIZE_KEY, TABLE_NAME_MAX_SIZE_DEFAULT
        )
        col_name_max_size = db_mapping.get(
            DB_MAPPING_COL_NAME_MAX_SIZE_KEY, COL_NAME_MAX_SIZE_DEFAULT
        )
        table_name_suffix = db_mapping.get(
            DB_MAPPING_TABLE_NAME_SUFFIX_KEY, TABLE_NAME_SUFFIX_DEFAULT
        )
        col_name_suffix = db_mapping.get(
            DB_MAPPING_COL_NAME_SUFFIX_KEY, COL_NAME_SUFFIX_DEFAULT
        )

        timestamp_col_info = db_mapping.get(DB_MAPPING_TIMESTAMP_COL_KEY, {})
        timestamp_col_name = timestamp_col_info.get(
            DB_MAPPING_TIMESTAMP_COL_NAME_KEY, TIMESTAMP_COL_NAME_DEFAULT
        )
        timestamp_col_type = timestamp_col_info.get(
            DB_MAPPING_TIMESTAMP_COL_TYPE_KEY, TIMESTAMP_COL_TYPE_DEFAULT
        )
        self._timestamp_col_strftime = timestamp_col_info.get(
            DB_MAPPING_TIMESTAMP_COL_STRFTIME_KEY, TIMESTAMP_COL_STRFTIME_DEFAULT
        )
        if timestamp_col_info.get(
            DB_MAPPING_TIMESTAMP_COL_USE_GMT_KEY, TIMESTAMP_COL_USE_GMT_DEFAULT
        ):
            self._tz = timezone.utc
        else:
            self._tz = get_localzone()

        lmcrec_info_schema = lmcrec_schema.get(LMCREC_SCHEMA_INFO_KEY, {})
        instance_col_info = db_mapping.get(DB_MAPPING_INSTANCE_COL_KEY, {})
        instance_col_name = instance_col_info.get(
            DB_MAPPING_INSTANCE_COL_NAME_KEY, INSTANCE_COL_NAME_DEFAULT
        )
        instance_col_type = instance_col_info.get(
            DB_MAPPING_INSTANCE_COL_DB_TYPE_KEY, INSTANCE_COL_TYPE_DEFAULT
        )
        # instance_col_type will be evaluated as a f-string that may contain
        # `inst_max_size' withing some expression. Note that since inst_max_size
        # is not used explicitly, it must be excluded from autoflake check (#
        # noqa)
        inst_max_size = lmcrec_info_schema.get(
            LMCREC_SCHEMA_INFO_INST_MAX_SIZE_KEY, 0
        )  # noqa
        instance_col_type = eval('f"""' + instance_col_type + '"""')

        cols_prefix = [
            (timestamp_col_name, timestamp_col_type, False, None, None),
            (instance_col_name, instance_col_type, True, None, None),
        ]
        prime_col_names = [cp[0] for cp in cols_prefix]

        db_data_type_mapping = db_mapping.get(DB_DATA_TYPE_MAPPING_KEY, {})

        db_boolean_type_mapping = db_data_type_mapping.get(
            DB_DATA_TYPE_MAPPING_BOOLEAN_CATEGORY_KEY, {}
        )
        self._boolean_value_mapping = {
            False: db_boolean_type_mapping.get(
                DB_DATA_TYPE_MAPPING_FALSE_VALUE_KEY,
                BOOL_FALSE_VALUE_DEFAULT,
            ),
            True: db_boolean_type_mapping.get(
                DB_DATA_TYPE_MAPPING_TRUE_VALUE_KEY,
                BOOL_TRUE_VALUE_DEFAULT,
            ),
        }

        class_schemas = lmcrec_schema.get(LMCREC_SCHEMA_CLASSES_KEY, {})
        table_name_normalizer = SchemaNormalizer(
            max_len=table_name_max_size,
            suffix=table_name_suffix,
        )
        self.tables = dict()
        self.lmc_classes = dict()

        def numeric_db_type(
            category_mapping: dict,
            default: str,
            neg_vals: bool,
        ) -> str:
            return category_mapping.get(
                (
                    DB_DATA_TYPE_MAPPING_SIGNED_COL_TYPE_KEY
                    if neg_vals
                    else DB_DATA_TYPE_MAPPING_UNSIGNED_COL_TYPE_KEY
                ),
                category_mapping.get(DB_DATA_TYPE_MAPPING_COL_TYPE_KEY, default),
            )

        for class_name, class_schema in class_schemas.items():
            table_name = table_name_normalizer(class_name)
            col_name_normalizer = SchemaNormalizer(
                max_len=col_name_max_size,
                reserved_names=prime_col_names,
                suffix=col_name_suffix,
            )
            cols = []
            for var_name, var_schema in class_schema.items():
                var_type = var_schema.get(LMCREC_SCHEMA_CLASS_VAR_TYPE_KEY)
                neg_vals = var_schema.get(LMCREC_SCHEMA_CLASS_VAR_NEG_VALS_KEY, False)
                col_name = col_name_normalizer(var_name)
                db_type_category = lmc_var_type_to_category_key.get(var_type)
                category_mapping = db_data_type_mapping.get(db_type_category, {})
                needs_collation = False
                if db_type_category == DB_DATA_TYPE_MAPPING_NUMERIC_CATEGORY_KEY:
                    col_type = numeric_db_type(
                        category_mapping, NUMERIC_COL_TYPE_DEFAULT, neg_vals
                    )
                elif (
                    db_type_category == DB_DATA_TYPE_MAPPING_LARGE_NUMERIC_CATEGORY_KEY
                ):
                    col_type = numeric_db_type(
                        category_mapping, LARGE_NUMERIC_COL_TYPE_DEFAULT, neg_vals
                    )
                elif db_type_category == DB_DATA_TYPE_MAPPING_STRING_CATEGORY_KEY:
                    col_type = category_mapping.get(
                        DB_DATA_TYPE_MAPPING_COL_TYPE_KEY,
                        STRING_COL_TYPE_DEFAULT,
                    )
                    # string col_type will be evaluated as a f-string that may
                    # contain `max_size' withing some expression. Note that
                    # since max_size is not used explicitly, it must be excluded
                    # from autoflake check (# noqa)
                    max_size = var_schema.get(
                        LMCREC_SCHEMA_CLASS_VAR_MAX_SIZE_KEY, 0
                    )  # noqa
                    col_type = eval('f"""' + col_type + '"""')
                    needs_collation = True
                elif db_type_category == DB_DATA_TYPE_MAPPING_BOOLEAN_CATEGORY_KEY:
                    col_type = category_mapping.get(
                        DB_DATA_TYPE_MAPPING_COL_TYPE_KEY,
                        BOOLEAN_COL_TYPE_DEFAULT,
                    )
                else:
                    raise RuntimeError(
                        "unsupported category: "
                        f"class: {class_name!r}, var: {var_name!r}, type: {var_type}"
                    )
                cols.append((col_name, col_type, needs_collation, var_name, var_type))

            cols = cols_prefix + sorted(cols, key=lambda c: c[0].lower())

            lmcrec_class_vars = dict()
            for i, (_, _, _, var_name, var_type) in enumerate(cols):
                if var_name is not None:
                    lmcrec_class_vars[var_name] = (var_type, i)
            self.lmc_classes[class_name] = (table_name, lmcrec_class_vars)
            self.tables[table_name] = [col[:3] for col in cols]

        csv_mapping = db_mapping.get(DB_MAPPING_CSV_KEY, dict())
        self.csv_dialect = export_csv_dialect(
            reference=csv_mapping.get(DB_MAPPING_CSV_DIALECT_KEY, CSV_DIALECT_DEFAULT),
            **csv_mapping.get(DB_MAPPING_CSV_DIALECT_PARAMS_KEY, dict()),
        )
        self.csv_include_header = csv_mapping.get(
            DB_MAPPING_CSV_INCLUDE_HEADER_KEY,
            CSV_INCLUDE_HEADER_DEFAULT,
        )
        self.csv_max_rows_per_file = csv_mapping.get(
            DB_MAPPING_CSV_MAX_ROWS_PER_FILE_KEY,
            CSV_MAX_LINES_PER_FILE_DEFAULT,
        )

        bcp_fmt_mapping = db_mapping.get(DB_MAPPING_BCP_FMT_KEY, dict())
        self.bcp_version = bcp_fmt_mapping.get(
            DB_MAPPING_BCP_FMT_VERSION_KEY, BCP_FMT_VERSION_DEFAULT
        )
        self.bcp_host_data_type = bcp_fmt_mapping.get(
            DB_MAPPING_BCP_FMT_HOST_DATA_TYPE_KEY, BCP_FMT_HOST_DATA_TYPE_DEFAULT
        )
        self.bcp_host_data_length = bcp_fmt_mapping.get(
            DB_MAPPING_BCP_FMT_HOST_DATA_LENGTH_KEY, BCP_FMT_HOST_DATA_LENGTH_DEFAULT
        )
        self.bcp_string_collation = bcp_fmt_mapping.get(
            DB_MAPPING_BCP_FMT_STRING_COLLATION_KEY, BCP_FMT_STRING_COLLATION_DEFAULT
        )

    def datetime_from_ts(self, ts: Optional[float] = None) -> str:
        if ts is None:
            ts = time.time()
        return datetime.fromtimestamp(ts, tz=self._tz).strftime(
            self._timestamp_col_strftime
        )

    def bool_val(self, val):
        return self._boolean_value_mapping.get(bool(val), 0)


class CsvExportWriter:
    def __init__(
        self,
        out_dir: str,
        dialect: Union[str, csv.Dialect],
        header: Optional[List[str]] = None,
        compress_level: Optional[int] = Z_NO_COMPRESSION,
        max_rows_per_file: int = 0,
    ):
        self._out_dir = out_dir
        os.makedirs(self._out_dir, exist_ok=True)
        self._dialect = dialect
        self._header = header
        self._compress_level = (
            compress_level if compress_level is not None else compress_level
        )
        self._max_rows_per_file = max_rows_per_file
        self._row_no = 0
        self._batch_no = 0
        self._fh = None
        self._writer = None

    def write(self, row: Iterable[Any]):
        if self._writer is None:
            if self._max_rows_per_file > 0:
                cvs_file = f"{CSV_BATCH_FILE_PREFIX}-{self._batch_no:06d}{CSV_BATCH_FILE_SUFFIX}"
                self._batch_no += 1
            else:
                cvs_file = CSV_BATCH_FILE_PREFIX + CSV_BATCH_FILE_SUFFIX
            file_name = os.path.join(self._out_dir, cvs_file)
            if self._compress_level != Z_NO_COMPRESSION:
                file_name += GZIP_FILE_SUFFIX
                self._fh = gzip.open(
                    file_name, compresslevel=self._compress_level, mode="wt"
                )
            else:
                self._fh = open(file_name, mode="wt")
            self._writer = csv.writer(self._fh, dialect=self._dialect)
            self._row_no = 0
            if self._header:
                self._writer.writerow(self._header)
        self._writer.writerow(row)
        self._row_no += 1
        if self._max_rows_per_file > 0 and self._row_no >= self._max_rows_per_file:
            self._fh.close()
            self._fh = None
            self._writer = None

    def close(self):
        if self._fh is not None:
            self._fh.close()
            self._fh = None

    def __del__(self):
        self.close()


@dataclass
class LmcrecClassHandler:
    csv_writer: Optional[CsvExportWriter] = None
    no_of_cols: int = 0
    col_index_by_var_id: Optional[Dict[int, int]] = None
    is_bool_by_var_id: Optional[Dict[int, bool]] = None


LmcrecClassIgnore = LmcrecClassHandler()


class LmcrecExporter:
    def __init__(
        self,
        lmcrec_db_mapping: LmcrecDbMapping,
        out_dir: str,
        compress_level: int = Z_NO_COMPRESSION,
        class_selection: Optional[Iterable[str]] = None,
    ):
        self._lmcrec_db_mapping = lmcrec_db_mapping
        self._out_dir = out_dir
        self._compress_level = (
            compress_level if compress_level is not None else Z_NO_COMPRESSION
        )
        self._class_selection = set(class_selection) if class_selection else None
        self._table_csv_writers: Dict[str, CsvExportWriter] = dict()  # [name] -> writer
        self._closed = False
        self.used_classes = set()

    def _build_class_handler(
        self, query_state_cache: LmcrecQueryIntervalStateCache, class_id: int
    ) -> LmcrecClassHandler:
        lmcrec_db_mapping = self._lmcrec_db_mapping
        class_info = query_state_cache.class_by_id[class_id]
        class_name, var_info_by_id = class_info.name, class_info.var_info_by_id
        if self._class_selection and class_name not in self._class_selection:
            return LmcrecClassIgnore
        self.used_classes.add(class_name)
        table_name, var_mapping = lmcrec_db_mapping.lmc_classes[class_name]
        csv_writer = self._table_csv_writers.get(table_name)
        cols = lmcrec_db_mapping.tables[table_name]
        if csv_writer is None:
            table_out_dir = os.path.join(self._out_dir, EXPORT_DATA_SUB_DIR, table_name)
            if lmcrec_db_mapping.csv_include_header:
                header = [col_name for col_name, _ in cols]
            else:
                header = None
            csv_writer = CsvExportWriter(
                table_out_dir,
                dialect=lmcrec_db_mapping.csv_dialect,
                header=header,
                compress_level=self._compress_level,
                max_rows_per_file=lmcrec_db_mapping.csv_max_rows_per_file,
            )
            self._table_csv_writers[table_name] = csv_writer
        col_index_by_var_id = dict()
        is_bool_by_var_id = dict()
        for var_id, var_info in var_info_by_id.items():
            col_index_by_var_id[var_id] = var_mapping[var_info.name][1]
            is_bool_by_var_id[var_id] = var_info.var_type in {
                LmcVarType.BOOLEAN,
                LmcVarType.BOOLEAN_CONFIG,
            }
        return LmcrecClassHandler(
            csv_writer=csv_writer,
            no_of_cols=len(cols),
            col_index_by_var_id=col_index_by_var_id,
            is_bool_by_var_id=is_bool_by_var_id,
        )

    def _update_class_handler(
        self,
        query_state_cache: LmcrecQueryIntervalStateCache,
        class_id: int,
        class_handler: LmcrecClassHandler,
        *var_ids,
    ):
        class_info = query_state_cache.class_by_id[class_id]
        class_name, var_info_by_id = class_info.name, class_info.var_info_by_id
        lmcrec_db_mapping = self._lmcrec_db_mapping
        _, var_mapping = lmcrec_db_mapping.lmc_classes[class_name]
        col_index_by_var_id = class_handler.col_index_by_var_id
        is_bool_by_var_id = class_handler.is_bool_by_var_id
        for var_id in var_ids:
            var_info = var_info_by_id[var_id]
            col_index_by_var_id[var_id] = var_mapping[var_info.name][1]
            is_bool_by_var_id[var_id] = var_info.var_type in {
                LmcVarType.BOOLEAN,
                LmcVarType.BOOLEAN_CONFIG,
            }

    def next_scan_cb(self, query_state_cache: LmcrecQueryIntervalStateCache) -> bool:
        """Handle next scan callback"""

        # For new chain (re-)initialize the class_id based cache(s):
        if query_state_cache.new_chain:
            self._handler_by_id = dict()

        # Iterate through the instances and generate new rows:
        for inst_name, inst_cache_entry in query_state_cache.inst_by_name.items():
            class_id = inst_cache_entry.class_id
            class_handler = self._handler_by_id.get(class_id)
            if class_handler is None:
                class_handler = self._build_class_handler(query_state_cache, class_id)
                self._handler_by_id[class_id] = class_handler
            if class_handler is LmcrecClassIgnore:
                continue
            row = [None] * class_handler.no_of_cols
            row[0] = self._lmcrec_db_mapping.datetime_from_ts(query_state_cache.ts)
            row[1] = inst_name
            col_index_by_var_id = class_handler.col_index_by_var_id
            is_bool_by_var_id = class_handler.is_bool_by_var_id
            unmapped_var_ids = []
            vars = inst_cache_entry.vars
            for var_id, val in vars.items():
                col_index = col_index_by_var_id.get(var_id)
                if col_index is None:
                    unmapped_var_ids.append(var_id)
                else:
                    if is_bool_by_var_id[var_id]:
                        row[col_index] = self._lmcrec_db_mapping.bool_val(val)
                    else:
                        row[col_index] = val
            if unmapped_var_ids:
                self._update_class_handler(
                    query_state_cache, class_id, class_handler, *unmapped_var_ids
                )
                for var_id in unmapped_var_ids:
                    val = vars[var_id]
                    if is_bool_by_var_id[var_id]:
                        row[col_index] = self._lmcrec_db_mapping.bool_val(val)
                    else:
                        row[col_index] = val
            class_handler.csv_writer.write(row)
        return True

    def generate_bcp_fmt_files(self):
        if not self._table_csv_writers:
            return

        lmcrec_db_mapping = self._lmcrec_db_mapping

        # Separators should be `"' enclosed w/ Python style representation:
        csv_dialect = lmcrec_db_mapping.csv_dialect
        field_sep = '"' + repr(csv_dialect.delimiter)[1:-1] + '"'
        line_sep = '"' + repr(csv_dialect.lineterminator)[1:-1] + '"'

        bcp_version = lmcrec_db_mapping.bcp_version
        bcp_host_data_type = lmcrec_db_mapping.bcp_host_data_type
        bcp_host_data_length = lmcrec_db_mapping.bcp_host_data_length
        bcp_string_collation = lmcrec_db_mapping.bcp_string_collation
        want_collation = bcp_string_collation is not None

        for table_name, csv_writer in self._table_csv_writers.items():
            cols = lmcrec_db_mapping.tables[table_name]
            n_cols = len(cols)
            last_index = n_cols - 1
            out_dir = csv_writer._out_dir
            os.makedirs(out_dir, exist_ok=True)
            with open(os.path.join(out_dir, BCP_FMT_FILE), "wt") as f:
                print(bcp_version, file=f)
                print(n_cols, file=f)
                for i, (col_name, _, needs_collation) in enumerate(cols):
                    col_order = str(i + 1)
                    line = [
                        col_order,
                        f"{bcp_host_data_type} 0",
                        str(bcp_host_data_length),
                        field_sep if i < last_index else line_sep,
                        col_order,
                        col_name,
                    ]
                    if want_collation:
                        line.append(bcp_string_collation if needs_collation else "")
                    print(BCP_FILE_FIELD_SEP.join(line), file=f)

    def generate_sql_commands(self):
        sql_out_dir = os.path.join(self._out_dir, EXPORT_SQL_SUB_DIR)
        os.makedirs(sql_out_dir, exist_ok=True)
        lmcrec_db_mapping = self._lmcrec_db_mapping
        for class_name in self.used_classes:
            table_name, _ = lmcrec_db_mapping.lmc_classes[class_name]
            with open(os.path.join(sql_out_dir, f"create-{table_name}.sql"), "wt") as f:
                print(f"CREATE TABLE {table_name} (", file=f)
                cols = lmcrec_db_mapping.tables[table_name]
                last_index = len(cols) - 1
                maybe_comma = ","
                for i, (col_name, col_type, _) in enumerate(cols):
                    if i == last_index:
                        maybe_comma = ""
                    print(
                        f"{CREATE_TABLE_INDENT}{col_name} {col_type}{maybe_comma}",
                        file=f,
                    )
                print(")", file=f)

    def generate_mapping_info(self):
        lmcrec_db_mapping = self._lmcrec_db_mapping
        headers = ("LMC Class", "DB Table", "LMC Var", "DB Col", "LMC Type", "DB Type")
        empty_row = ("", "", "", "", "", "")
        mapping_info_file = os.path.join(self._out_dir, MAPPING_INFO_FILE)
        with open(mapping_info_file, "wt") as f:
            rows = []
            for class_name in sorted(self.used_classes, key=lambda c: c.lower()):
                table_name, var_mapping = lmcrec_db_mapping.lmc_classes[class_name]
                cols = lmcrec_db_mapping.tables[table_name]
                if rows:
                    # Intertable separation:
                    rows.append(SEPARATING_LINE)
                    rows.append(empty_row)
                    rows.append(empty_row)
                    rows.append(SEPARATING_LINE)
                rows.append(headers)
                rows.append(SEPARATING_LINE)
                # Start w/ the 2 cols that have no LMC counterpart:
                ts_col, ts_db_type = cols[0][:2]  # timestamp
                inst_col, inst_db_type = cols[1][:2]  # instance
                rows.append((class_name, table_name, "", ts_col, "", ts_db_type))
                rows.append(
                    (class_name, table_name, "", inst_col, "", inst_db_type)
                )  # timestamp
                for var_name in sorted(var_mapping, key=lambda v: v.lower()):
                    var_type, col_index = var_mapping[var_name]
                    col_name, col_type = cols[col_index][:2]
                    rows.append(
                        (class_name, table_name, var_name, col_name, var_type, col_type)
                    )
            print(tabulate(rows, tablefmt="simple"), file=f)

    def close(self):
        if not self._closed:
            self._closed = True
            self.generate_bcp_fmt_files()
            self.generate_sql_commands()
            self.generate_mapping_info()
            if self._table_csv_writers:
                for csv_writer in self._table_csv_writers.values():
                    csv_writer.close()
                self._table_csv_writers = None

    def __del__(self):
        self.close()


def main():
    parser = argparse.ArgumentParser(
        formatter_class=CustomWidthFormatter,
        description=description,
        parents=[get_file_selection_arg_parser()],
    )
    parser.add_argument(
        "-S",
        "--schema-file",
        help="""
        LmcrecSchema file (YAML format) created by inventory or merged from multiple
        inventory files.
        """,
    )
    parser.add_argument(
        "-X",
        "--db-mapping-file",
        help="""
        DB specific file (YAML format) to describe name and type mapping and
        output formatting.
        """,
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        metavar="OUTPUT_DIR",
        help=f"""
        Save the information under OUTPUT_DIR/TABLE directory. The directory
        will be created as needed. If OUTPUT_DIR is specified as "auto" then it
        will default to
        $LMCREC_RUNTIME/export/INST/FIRST_TIMESTAMP--LAST_TIMESTAMP
        """,
    )
    parser.add_argument(
        "-z",
        "--compress-level",
        nargs="?",
        type=int,
        const=Z_DEFAULT_COMPRESSION,
        help=f"""
        Indicate that the CSV files will be compressed and optionally set the
        compression level, if it other than
        Z_DEFAULT_COMPRESSION={Z_DEFAULT_COMPRESSION}. 
        """,
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help=f"""Display progress information""",
    )

    args = parser.parse_args()

    start_ts = time.time()

    record_files_dir, from_ts, to_ts = process_file_selection_args(args)

    with open(args.schema_file, "rt") as f:
        lmcrec_schema = yaml.safe_load(f)

    with open(args.db_mapping_file, "rt") as f:
        db_mapping = yaml.safe_load(f)

    lmcrec_db_mapping = LmcrecDbMapping(lmcrec_schema, db_mapping)

    output_dir = args.output_dir
    tmp_output_dir = None
    if output_dir == "auto":
        # First and last timestamps are not know until after the query: use a
        # temp dir and rename at the end:
        parent_output_dir = os.path.join(
            get_lmcrec_runtime(), "export", args.inst or "unknown"
        )
        tmp_output_dir = os.path.join(parent_output_dir, str(uuid4()))
        output_dir = tmp_output_dir

    query_state_cache = LmcrecQueryIntervalStateCache(
        record_files_dir,
        from_ts=from_ts,
        to_ts=to_ts,
        _verbose=args.verbose,
    )

    lmcrec_exporter = LmcrecExporter(
        lmcrec_db_mapping=lmcrec_db_mapping,
        out_dir=output_dir,
        compress_level=args.compress_level,
    )

    exit_code = 0
    try:
        ret_code = query_state_cache.run_with_cb(lmcrec_exporter.next_scan_cb)
        lmcrec_exporter.close()
        if ret_code != LmcrecScanRetCode.ATEOR:
            exit_code = 1
            print(
                f"Warning: query_state_cache returned {ret_code!r}, the results may be incomplete",
                file=sys.stderr,
            )
        if tmp_output_dir is not None:
            first_ts = query_state_cache.first_ts
            if first_ts is None:
                first_ts = from_ts
            last_ts = query_state_cache.last_ts
            if last_ts is None:
                last_ts = to_ts
            output_dir = os.path.join(
                parent_output_dir,
                "--".join(
                    [
                        format_ts(int(first_ts)) if first_ts is not None else "oldest",
                        format_ts(int(last_ts)) if last_ts is not None else "newest",
                    ]
                ),
            )
            if os.path.exists(output_dir):
                rmtree(output_dir)
            os.rename(tmp_output_dir, output_dir)
            tmp_output_dir = None
        print(f"Results saved under {output_dir!r}", file=sys.stderr)

        d_time = time.time() - start_ts
        print(f"Completed in {d_time:.06f}s", file=sys.stderr)
    finally:
        if tmp_output_dir is not None:
            rmtree(tmp_output_dir)

    return exit_code


if __name__ == "__main__":
    sys.exit(main())
